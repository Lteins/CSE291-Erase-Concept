{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junda/miniconda3/envs/sd/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from StableDiffuser import StableDiffuser\n",
    "from finetuning import FineTunedModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from PIL import Image\n",
    "from diffusers import AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_artists):\n",
    "    import torchvision\n",
    "    # transfer learning on top of ResNet (only replacing final FC layer)\n",
    "    # model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "    model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "    for param in model_conv.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model_conv.fc.in_features\n",
    "    model_conv.fc = nn.Linear(num_ftrs, num_artists)\n",
    "    # load the pre-trained weights\n",
    "    model_conv.load_state_dict(torch.load('./detector/artist/artist_ckp/state_dict.dat.von_gogh'))\n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from StableDiffuser import StableDiffuser\n",
    "from finetuning import FineTunedModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from PIL import Image\n",
    "from diffusers import AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RGBConverter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RGBConverter, self).__init__()\n",
    "        # Magic number used in the detector\n",
    "        mean_resnet = np.array([0.485, 0.456, 0.406])\n",
    "        std_resnet = np.array([0.229, 0.224, 0.225])\n",
    "        self.val_transform = T.Compose([T.Resize(224), T.Normalize(mean_resnet, std_resnet)])\n",
    "    \n",
    "    def toRGB(self, RGBA, background=(255,255,255)):\n",
    "        _, D, R, C = RGBA.shape\n",
    "        if D == 3:\n",
    "            return RGBA\n",
    "        RGB = torch.zeros((1, 3, R, C), dtype=torch.float32)\n",
    "        R, G, B, A = RGBA[0].split(1, dim=0)\n",
    "        A = A.float() / 255\n",
    "        RGB[0, 0,:,:] = R.squeeze() * A.squeeze() + (1 - A.squeeze()) * background[0]\n",
    "        RGB[0, 1,:,:] = G.squeeze() * A.squeeze() + (1 - A.squeeze()) * background[1]\n",
    "        RGB[0, 2,:,:] = B.squeeze() * A.squeeze() + (1 - A.squeeze()) * background[2]\n",
    "        return RGB\n",
    "\n",
    "    def forward(self, input):\n",
    "        min = torch.min(input.detach())\n",
    "        max = torch.max(input.detach())\n",
    "        input = (input-min)/(max-min)*255\n",
    "        #input = self.toRGB(input)\n",
    "        input = self.val_transform(input.squeeze())\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class ArtModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ArtModel, self).__init__()\n",
    "        self.rgb = RGBConverter()\n",
    "        self.vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(\"cuda:1\")\n",
    "        self.classifier = create_model(5)\n",
    "        self.classifier.eval()\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        x = self.vae.decode(1 / self.vae.config.scaling_factor * x).sample\n",
    "        x = self.rgb(x)\n",
    "        x = self.classifier(x.unsqueeze(0))\n",
    "        return x\n",
    "\n",
    "class ObjectDetector():\n",
    "    def __init__(self):\n",
    "        self.dtype = torch.FloatTensor\n",
    "        if (torch.cuda.is_available()):\n",
    "            self.dtype = torch.cuda.FloatTensor\n",
    "        # transfer learning on top of ResNet (only replacing final FC layer)\n",
    "        self.model = ArtModel()\n",
    "        \n",
    "        self.model.to(\"cuda:1\")\n",
    "        self.device = \"cuda:1\"\n",
    "        # self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "    def get_input_grad(self, x): \n",
    "        x_var = Variable(x.type(self.dtype).to(self.device), requires_grad=True)\n",
    "        resnet_output = self.model(x_var)\n",
    "        prob = resnet_output[0][2]\n",
    "        prob.backward()\n",
    "        return x_var.grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Alfred Sisley\"\n",
    "modules = \".*attn2$\"\n",
    "iterations=200\n",
    "negative_guidance=1\n",
    "lr=0.015\n",
    "save_path=\"tmp/test\"\n",
    "freeze_modules=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRAIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junda/miniconda3/envs/sd/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Finetuning unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2\n",
      "=> Finetuning unet.mid_block.attentions.0.transformer_blocks.0.attn2\n",
      "Begin pbar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# kwargs = dict(locals())\n",
    "# print(f\"train kwargs: {kwargs}\")\n",
    "print(\"BEGIN TRAIN\")  \n",
    "nsteps = 50\n",
    "\n",
    "diffuser = StableDiffuser(scheduler='DDIM').to('cuda:1')\n",
    "diffuser.train()\n",
    "\n",
    "\n",
    "finetuner = FineTunedModel(diffuser, modules, frozen_modules=freeze_modules)\n",
    "# finetuner = FineTunedModel.from_checkpoint(diffuser, \"models/vangogh.pt\")\n",
    "\n",
    "params = list(finetuner.parameters())\n",
    "criteria = torch.nn.MSELoss()\n",
    "\n",
    "print(\"Begin pbar\")\n",
    "pbar = tqdm(range(iterations))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # neutral_text_embeddings = diffuser.get_text_embeddings([''],n_imgs=1)\n",
    "    positive_text_embeddings = diffuser.get_text_embeddings([prompt],n_imgs=1)\n",
    "\n",
    "# del diffuser.vae\n",
    "# del diffuser.text_encoder\n",
    "# del diffuser.tokenizer\n",
    "del diffuser.safety_checker\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junda/miniconda3/envs/sd/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/junda/miniconda3/envs/sd/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# del detector, optimizer\n",
    "detector = ObjectDetector()\n",
    "optimizer = torch.optim.SGD(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuser.train()\n",
    "for i in pbar:\n",
    "    with torch.no_grad():\n",
    "        diffuser.set_scheduler_timesteps(60)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        diffuse_iter = torch.randint(1, nsteps-1, (1,)).item()\n",
    "\n",
    "        latents = diffuser.get_initial_latents(1, 512, 1)\n",
    "        # print(\"LATENT SIZE: \", latents.size())\n",
    "        with finetuner:\n",
    "            latents_steps, _ = diffuser.diffusion(\n",
    "                latents,\n",
    "                positive_text_embeddings,\n",
    "                start_iteration=0,\n",
    "                end_iteration=diffuse_iter,\n",
    "                guidance_scale=3,\n",
    "                show_progress=False,\n",
    "            )\n",
    "        \n",
    "        # diffuser.set_scheduler_timesteps(diffuse_iter+1)\n",
    "        # diffuse_iter = int(diffuse_iter / nsteps * 1000)\n",
    "        # ref_latents = diffuser.predict_noise(diffuse_iter, latents_steps[0], positive_text_embeddings, guidance_scale=1)\n",
    "        with finetuner:\n",
    "            ref_latents = diffuser.predict_noise(diffuse_iter, latents_steps[0], positive_text_embeddings, guidance_scale=1)\n",
    "\n",
    "\n",
    "    with finetuner:\n",
    "        negative_latents = diffuser.predict_noise(diffuse_iter, latents_steps[0], positive_text_embeddings, guidance_scale=1)\n",
    "    \n",
    "    # y = torch.tensor([53]) # label of Von Gogh\n",
    "    \n",
    "    # dump input_x to a file\n",
    "    # torch.save(input_x, \"detector/artist/test_vg/input_{}.pt\".format(i))\n",
    "    # generated_images.append(input_x)\n",
    "    \n",
    "    # print(torch.norm(detector_grad))\n",
    "    # print(torch.norm(neutral_latents))\n",
    "    # print(torch.norm(negative_latents))\n",
    "    input_x = latents_steps[0]    \n",
    "    detector_grad = detector.get_input_grad(input_x)        \n",
    "    loss = criteria(negative_latents.float(), ref_latents.detach().float() + 10*(detector_grad)) #loss = criteria(e_n, e_0) works the best try 5000 epochs\n",
    "    loss.backward()\n",
    "    print(\"Loss function\")\n",
    "    print(loss.item())\n",
    "    print(\"Gradient Scale\")\n",
    "    gradient = params[0].grad\n",
    "    print(torch.norm(gradient))        \n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # if i % 10 == 0 and i != 0:\n",
    "    #     now_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    #     torch.save(\n",
    "    #         finetuner.state_dict(), \n",
    "    #         save_path + f'_checkpoint_{i}_{now_str}.pt'\n",
    "    #     )\n",
    "\n",
    "now_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "torch.save(finetuner.state_dict(), save_path + f'_{now_str}.pt')\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del finetuner, diffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuser = StableDiffuser(scheduler='DDIM').to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtuner = FineTunedModel.from_checkpoint(diffuser, \"detector_art/test_checkpoint_10_20231130_192118.pt\").eval().to(\"cuda:1\")\n",
    "# testtuner = FineTunedModel.from_checkpoint(diffuser, \"models/vangogh.pt\").eval().to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.manual_seed(30)\n",
    "with testtuner:\n",
    "    images = diffuser(\n",
    "        \"Von Gohh Painting\",\n",
    "        n_steps=50,\n",
    "        n_imgs=10,\n",
    "        generator=generator,\n",
    "        noise_level=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:1\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_original_van = 0\n",
    "for i in range(10):\n",
    "    if (torch.argmax(model(transform(images[i][0]).unsqueeze(0).float().cuda().to(\"cuda:1\"))) == 4):\n",
    "        num_original_van += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_original_van"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "sd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
